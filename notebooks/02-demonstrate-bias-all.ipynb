{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf3002f-3004-4fc6-90f1-0af07682b26e",
   "metadata": {},
   "source": [
    "# Demonstrate Positional Bias\n",
    "\n",
    "Our goal here is to quantify positional bias inherrent in our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5dde05-1cdb-4b1b-9624-e848d0211089",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c16667-139d-4211-8ace-4c48419b614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from time import perf_counter_ns as timer\n",
    "from uuid import uuid4\n",
    "\n",
    "from pandas import DataFrame\n",
    "import rich\n",
    "from rich.table import Table\n",
    "from tqdm import tqdm\n",
    "\n",
    "import llm_mcq_bias as lmb\n",
    "from llm_mcq_bias.datasets.mmlu import Evaluation, OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972849e-862d-44be-bdff-6299e8e152fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_table(rows, title: str | None = None):\n",
    "    table = Table(*[k for k in rows[0]], title=title, box=rich.box.SIMPLE)\n",
    "    for row in rows:\n",
    "        table.add_row(*[str(v) for v in row.values()])\n",
    "    rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9ede7-ba69-4129-8c04-e1c5cda116f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path(\"../\")\n",
    "datasets_path = project_path / \".build\" / \"datasets\"\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b09bb2-0aeb-436a-a111-0fc4b0c5899f",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d6fea-a824-4fdc-952a-275fdb0d94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider config\n",
    "providers = {\n",
    "    \"ollama\": {\n",
    "        \"model\": \"llama3.2:3b-instruct-fp16\",\n",
    "        \"options\": {\n",
    "            \"num_predict\": 10,  # Limit output tokens to avoid waiting for invalid responses\n",
    "            \"top_k\": 1,  # Disable token sampling\n",
    "        },\n",
    "        \"generator_factory\": lmb.models.ollama,\n",
    "    },\n",
    "    \"openai\": {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"options\": {\n",
    "            \"max_tokens\": 10,  # Limit output tokens to avoid waiting for invalid responses\n",
    "            \"temperature\": 0,  # Disable token sampling\n",
    "        },\n",
    "        \"generator_factory\": lmb.models.openai,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Selected provider\n",
    "provider = \"openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e6367-2ec9-40b3-ab6a-ef10259005a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_epochs = 10\n",
    "\n",
    "# Number of questions to sample\n",
    "n_questions = 160\n",
    "\n",
    "# Number of parallel requests\n",
    "n_jobs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac59e38-4538-416b-bc05-a441ff8a36bc",
   "metadata": {},
   "source": [
    "# Demonstrate Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74a8b5-342f-4fe3-820d-a58ce0f06569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example questions\n",
    "examples = lmb.datasets.mmlu.load_dataset(datasets_path, segment=\"dev\")\n",
    "\n",
    "# Debias example answer distribution\n",
    "examples = lmb.datasets.mmlu.normalize_example_answers(examples)\n",
    "\n",
    "# Load test questions\n",
    "questions = lmb.datasets.mmlu.load_dataset(datasets_path, segment=\"test\")\n",
    "\n",
    "# Initialize thread pool\n",
    "executor = ThreadPoolExecutor(max_workers=n_jobs)\n",
    "\n",
    "# Create generator from provider\n",
    "generator_factory = providers[provider][\"generator_factory\"]\n",
    "model = providers[provider][\"model\"]\n",
    "options = providers[provider][\"options\"]\n",
    "generator = partial(generator_factory, model=model, options=options)\n",
    "\n",
    "print(f\"Configured {provider} provider: {model}, {options}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd801b-7838-4763-ad8e-564fc6655a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mcq(mcq):\n",
    "    # Generate prompt\n",
    "    prompt = lmb.datasets.mmlu.generate_prompt(examples, mcq)\n",
    "\n",
    "    # Generate answer\n",
    "    response = generator(prompt=prompt)\n",
    "\n",
    "    # Evaluate response\n",
    "    return lmb.datasets.mmlu.evaluate_response(mcq, response)\n",
    "\n",
    "\n",
    "def benchmark(\n",
    "    description: str,\n",
    "    *,\n",
    "    examples: DataFrame,\n",
    "    questions: DataFrame,\n",
    "):\n",
    "    n = len(questions)\n",
    "\n",
    "    start_time = timer()\n",
    "\n",
    "    # Answer and evaluate each question\n",
    "    futures = [executor.submit(process_mcq, mcq) for _, mcq in questions.iterrows()]\n",
    "\n",
    "    # Collect results\n",
    "    correct, errors = 0, 0\n",
    "    for future in tqdm(as_completed(futures), total=n, desc=description):\n",
    "        evaluation = future.result()\n",
    "        if evaluation is Evaluation.CORRECT:\n",
    "            correct += 1\n",
    "        elif evaluation is Evaluation.ERROR:\n",
    "            errors += 1\n",
    "\n",
    "    duration = timer() - start_time\n",
    "\n",
    "    # Derive metrics\n",
    "    metrics = {\n",
    "        \"n\": n,\n",
    "        \"correct\": correct,\n",
    "        \"errors\": errors,\n",
    "        \"accuracy\": correct / (n - errors),\n",
    "        \"error_rate\": errors / n,\n",
    "        \"rps\": 1000000000 * n / duration,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725062b-af28-465e-9e63-ed811d778d04",
   "metadata": {},
   "source": [
    "### Verify Stable Benchmark Results\n",
    "\n",
    "Let's make sure our benchmark process produces consistent results when run against the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc163e0-0718-4d08-b107-08424c406df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions\n",
    "selected_questions = questions.sample(n=16)\n",
    "\n",
    "# Debias answer distribution\n",
    "selected_questions = lmb.datasets.mmlu.normalize_question_answers(selected_questions)\n",
    "\n",
    "# Plot answer distribution\n",
    "selected_questions.answer.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbe10b-2249-435a-950c-d716b4ab8fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print example prompt\n",
    "print(lmb.datasets.mmlu.generate_prompt(examples, selected_questions.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1ff00-a43e-41d1-9c89-6e6642f09620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rows = []\n",
    "for i in range(3):\n",
    "    # Run benchmark\n",
    "    metrics = benchmark(\n",
    "        f\"epoch {i}\",\n",
    "        examples=examples,\n",
    "        questions=selected_questions,\n",
    "    )\n",
    "\n",
    "    rows.append(metrics)\n",
    "\n",
    "print_table(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c39e34-9333-4798-b059-b55748fbcade",
   "metadata": {},
   "source": [
    "## Estimate Positional Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af877f4d-2cc3-40b3-8a92-3ad28e5e0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Repeat over multiple iterations\n",
    "rows = []\n",
    "for _ in range(n_epochs):\n",
    "    # Sample questions\n",
    "    selected_questions = questions.sample(n=n_questions)\n",
    "\n",
    "    # Debias answer distribution\n",
    "    selected_questions = lmb.datasets.mmlu.normalize_question_answers(\n",
    "        selected_questions\n",
    "    )\n",
    "\n",
    "    # Initialize metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # Record performance w/ original data\n",
    "    metrics[\"uniform\"] = benchmark(\n",
    "        \"uniform\",\n",
    "        examples=examples,\n",
    "        questions=selected_questions,\n",
    "    )\n",
    "\n",
    "    # Record performance w/ answers shifted to each position\n",
    "    for option in OPTIONS:\n",
    "        # Swap answers to selected option\n",
    "        q = lmb.datasets.mmlu.swap_options(selected_questions, option)\n",
    "\n",
    "        metrics[option] = benchmark(\n",
    "            option,\n",
    "            examples=examples,\n",
    "            questions=q,\n",
    "        )\n",
    "\n",
    "    rows.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf71dd-01e1-42e7-ab66-798960c4b2e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "table = Table(\"uniform\", \"A\", \"B\", \"C\", \"D\", box=rich.box.SIMPLE)\n",
    "for row in rows:\n",
    "    baseline = row[\"uniform\"][\"accuracy\"]\n",
    "    offsets = {k: row[k][\"accuracy\"] - baseline for k in OPTIONS}\n",
    "    colors = {option: \"black\" for option in OPTIONS}\n",
    "    colors |= {option: \"red\" for option in OPTIONS if offsets[option] <= -0.05}\n",
    "    colors |= {option: \"green\" for option in OPTIONS if offsets[option] >= 0.05}\n",
    "    table.add_row(\n",
    "        f\"{baseline:0.2f}\",\n",
    "        f\"[{colors['A']}]{offsets['A']:0.2f}[/{colors['A']}]\",\n",
    "        f\"[{colors['B']}]{offsets['B']:0.2f}[/{colors['B']}]\",\n",
    "        f\"[{colors['C']}]{offsets['C']:0.2f}[/{colors['C']}]\",\n",
    "        f\"[{colors['D']}]{offsets['D']:0.2f}[/{colors['D']}]\",\n",
    "    )\n",
    "\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a7c2e-c8f3-465b-b68e-8f78f88e9171",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a8cfc-cc6e-410f-8bec-264b152f5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"results/demonstrate-bias-{model}-{n_questions}-{n_epochs}-{uuid4().hex}.json\")\n",
    "path.write_text(json.dumps(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4207d-7cf3-4eaf-9291-1fbfcc5729fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e3f48-eb24-4c3a-a4bd-38341b4f7dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
